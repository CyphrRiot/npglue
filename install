#!/bin/bash
# NPGlue - DeepSeek-R1 Installation Script
# Simple, beautiful, and memory-safe installation

set -e

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m'

clear
echo -e "${BLUE}â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®${NC}"
echo -e "${BLUE}â”‚             ğŸš€ NPGlue Installer             â”‚${NC}"
echo -e "${BLUE}â”‚      DeepSeek-R1 + OpenVINO + Goose        â”‚${NC}"
echo -e "${BLUE}â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯${NC}"
echo
echo -e "${CYAN}This will install everything you need for local AI coding assistance${NC}"
echo

# System checks
echo -e "${BLUE}ğŸ“‹ Checking system requirements...${NC}"
TOTAL_MEM_GB=$(free -g | awk '/^Mem:/{print $2}')
AVAILABLE_GB=$(df . | awk 'NR==2{printf "%.0f", $4/1024/1024}')

if [ "$TOTAL_MEM_GB" -lt 8 ]; then
    echo -e "${RED}âŒ Need at least 8GB RAM (found ${TOTAL_MEM_GB}GB)${NC}"
    echo -e "${YELLOW}ğŸ’¡ INT4-AWQ model is memory efficient but still needs 8GB+${NC}"
    exit 1
fi
if [ "$AVAILABLE_GB" -lt 15 ]; then
    echo -e "${RED}âŒ Need at least 15GB disk space${NC}"
    exit 1
fi
echo -e "${GREEN}âœ… System OK: ${TOTAL_MEM_GB}GB RAM, ${AVAILABLE_GB}GB space${NC}"

# Install system dependencies
echo -e "\n${BLUE}ğŸ“¦ Installing system packages...${NC}"
sudo pacman -S --needed --noconfirm python python-pip base-devel cmake git intel-compute-runtime
echo -e "${GREEN}âœ… System packages installed${NC}"

# Create clean Python environment
echo -e "\n${BLUE}ğŸ Setting up Python environment...${NC}"
[ -d "openvino-env" ] && rm -rf openvino-env
python -m venv openvino-env
source openvino-env/bin/activate
pip install --upgrade pip wheel

# Install AI packages
echo -e "${BLUE}ğŸ¤– Installing AI packages...${NC}"
pip install "openvino>=2024.0.0" "openvino-tokenizers>=2024.0.0" optimum-intel transformers
pip install torch --index-url https://download.pytorch.org/whl/cpu
pip install fastapi uvicorn psutil huggingface-hub
echo -e "${GREEN}âœ… AI packages installed${NC}"

# Download model (safe, no loading)
echo -e "\n${BLUE}ğŸ“¥ Downloading DeepSeek-R1 model...${NC}"
mkdir -p models
if [ -f "models/deepseek-r1-int4-awq/DeepSeek-R1-0528-Qwen3-8B-int4_asym-awq-se-ov/openvino_model.bin" ]; then
    echo -e "${GREEN}âœ… Model already downloaded${NC}"
else
    echo -e "${CYAN}Downloading optimized INT4-AWQ OpenVINO model...${NC}"
    echo -e "${PURPLE}Using INT4-AWQ: 95%+ quality, faster speed, less RAM${NC}"
    python -c "
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id='Echo9Zulu/DeepSeek-R1-0528-Qwen3-8B-OpenVINO',
    allow_patterns=['DeepSeek-R1-0528-Qwen3-8B-int4_asym-awq-se-ov/*'],
    local_dir='models/deepseek-r1-int4-awq',
    local_dir_use_symlinks=False
)
print('âœ… INT4-AWQ model downloaded successfully')
print('ğŸš€ Faster inference with preserved reasoning capability!')
"
fi

# Memory-safe verification (no model loading)
echo -e "\n${BLUE}ğŸ§ª Verifying installation...${NC}"
python -c "
import openvino
import transformers
print('âœ… OpenVINO:', openvino.__version__)
print('âœ… Transformers:', transformers.__version__)
print('âœ… Available devices:', openvino.Core().available_devices)
print('âœ… All packages working correctly')
"

# Create startup script
cat > start_server.sh << 'EOF'
#!/bin/bash
cd "$(dirname "$0")"
source openvino-env/bin/activate
python server_production.py
EOF
chmod +x start_server.sh

# CPU optimization
echo -e "\n${BLUE}âš¡ Optimizing CPU performance...${NC}"
sudo ./boost_cpu.sh

# Beautiful completion message
clear
echo -e "${GREEN}â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®${NC}"
echo -e "${GREEN}â”‚           ğŸ‰ INSTALLATION COMPLETE!         â”‚${NC}"
echo -e "${GREEN}â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯${NC}"
echo
echo -e "${BLUE}ğŸ“‹ NPGlue is ready! Here's what to do next:${NC}"
echo
echo -e "${YELLOW}1. Start the AI server:${NC}"
echo -e "   ${CYAN}./start_server.sh${NC}"
echo
echo -e "${YELLOW}2. Configure Goose (SAFE method):${NC}"
echo -e "   ${CYAN}# Check if you have existing Goose config:${NC}"
echo -e "   ${CYAN}ls ~/.config/goose/config.yaml${NC}"
echo
echo -e "   ${CYAN}# If NO existing config:${NC}"
echo -e "   ${CYAN}mkdir -p ~/.config/goose${NC}"
echo -e "   ${CYAN}cp goose_config_example.yaml ~/.config/goose/config.yaml${NC}"
echo
echo -e "   ${CYAN}# If you HAVE existing config, add these lines:${NC}"
echo -e "${PURPLE}   provider: openai${NC}"
echo -e "${PURPLE}   model: deepseek-r1-openvino${NC}"
echo -e "${PURPLE}   api_base: http://localhost:8000/v1${NC}"
echo -e "${PURPLE}   api_key: local-key${NC}"
echo -e "   ${CYAN}# (don't overwrite your existing settings!)${NC}"
echo
echo -e "${YELLOW}3. Update Zed settings:${NC}"
echo -e "   ${CYAN}# Add to ~/.config/zed/settings.json:${NC}"
echo -e "${PURPLE}   \"language_models\": {${NC}"
echo -e "${PURPLE}     \"openai\": {${NC}"
echo -e "${PURPLE}       \"api_url\": \"http://localhost:8000/v1\",${NC}"
echo -e "${PURPLE}       \"api_key\": \"local-key\",${NC}"
echo -e "${PURPLE}       \"version\": \"1\",${NC}"
echo -e "${PURPLE}       \"available_models\": [${NC}"
echo -e "${PURPLE}         {${NC}"
echo -e "${PURPLE}           \"name\": \"deepseek-r1-openvino\",${NC}"
echo -e "${PURPLE}           \"display_name\": \"DeepSeek-R1 Local\",${NC}"
echo -e "${PURPLE}           \"max_tokens\": 32768,${NC}"
echo -e "${PURPLE}           \"supports_tools\": true${NC}"
echo -e "${PURPLE}         }${NC}"
echo -e "${PURPLE}       ]${NC}"
echo -e "${PURPLE}     }${NC}"
echo -e "${PURPLE}   }${NC}"
echo -e "   ${CYAN}# Then set default model in agent section:${NC}"
echo -e "${PURPLE}   \"agent\": {${NC}"
echo -e "${PURPLE}     \"default_model\": {${NC}"
echo -e "${PURPLE}       \"provider\": \"openai\",${NC}"
echo -e "${PURPLE}       \"model\": \"deepseek-r1-openvino\"${NC}"
echo -e "${PURPLE}     }${NC}"
echo -e "${PURPLE}   }${NC}"
echo
echo -e "${YELLOW}4. Test the installation:${NC}"
echo -e "   ${CYAN}curl http://localhost:8000/health${NC}"
echo -e "   ${CYAN}python test_installation.py${NC}"
echo
echo -e "${GREEN}ğŸ“Š Performance expectations:${NC}"
echo -e "   â€¢ Speed: ${GREEN}20-30+ tokens/sec${NC} (INT4-AWQ optimized)"
echo -e "   â€¢ Memory: ${GREEN}~6-8GB${NC} (reduced from FP16)"
echo -e "   â€¢ Model: ${GREEN}DeepSeek-R1 INT4-AWQ (~5.6GB)${NC}"
echo -e "   â€¢ Quality: ${GREEN}95%+ of FP16 capability${NC}"
echo
echo -e "${BLUE}ğŸ’¡ Tips:${NC}"
echo -e "   â€¢ First generation is slower (warmup)"
echo -e "   â€¢ Use 'source openvino-env/bin/activate' to activate environment"
echo -e "   â€¢ Server runs on http://localhost:8000"
echo
echo -e "${GREEN}âœ¨ Happy coding with your local AI assistant! âœ¨${NC}"
