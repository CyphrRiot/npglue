#!/bin/bash
# NPGlue - Qwen3 Installation Script
# Simple, beautiful, and memory-safe installation

set -e

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m'

clear
echo -e "${BLUE}‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ${NC}"
echo -e "${BLUE}‚îÇ             üöÄ NPGlue Installer             ‚îÇ${NC}"
echo -e "${BLUE}‚îÇ        Qwen3 + OpenVINO + Goose             ‚îÇ${NC}"
echo -e "${BLUE}‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ${NC}"
echo
echo -e "${CYAN}This will install everything you need for local AI coding assistance${NC}"
echo

# System checks
echo -e "${BLUE}üìã Checking system requirements...${NC}"
TOTAL_MEM_GB=$(free -g | awk '/^Mem:/{print $2}')
AVAILABLE_GB=$(df . | awk 'NR==2{printf "%.0f", $4/1024/1024}')

# Check for Intel NPU/iGPU
echo -e "${BLUE}üîç Checking Intel hardware compatibility...${NC}"
HAS_INTEL_GPU=false
HAS_INTEL_NPU=false

# Check for Intel integrated graphics
if lspci | grep -i "intel.*graphics" >/dev/null 2>&1; then
    HAS_INTEL_GPU=true
    echo -e "${GREEN}‚úÖ Intel integrated graphics detected${NC}"
else
    echo -e "${YELLOW}‚ö†Ô∏è  No Intel integrated graphics detected${NC}"
fi

# Check for Intel NPU (Neural Processing Unit)
if lspci | grep -i "intel.*npu\|intel.*neural" >/dev/null 2>&1; then
    HAS_INTEL_NPU=true
    echo -e "${GREEN}‚úÖ Intel NPU detected - hardware acceleration available${NC}"
elif [ -d "/sys/class/drm/renderD*" ] && ls /sys/class/drm/renderD* 2>/dev/null | grep -q .; then
    # Check for Intel compute units via render devices
    echo -e "${BLUE}üîç Checking for Intel compute acceleration...${NC}"
    HAS_INTEL_GPU=true
    echo -e "${GREEN}‚úÖ Intel compute acceleration available${NC}"
else
    echo -e "${YELLOW}‚ö†Ô∏è  No Intel NPU detected${NC}"
fi

# Provide guidance based on hardware
if [ "$HAS_INTEL_NPU" = true ]; then
    echo -e "${CYAN}üí° Your system has Intel NPU - excellent for AI acceleration!${NC}"
elif [ "$HAS_INTEL_GPU" = true ]; then
    echo -e "${CYAN}üí° Your system has Intel GPU - good for OpenVINO acceleration${NC}"
else
    echo -e "${YELLOW}üí° No Intel NPU/GPU detected - will use CPU-only mode${NC}"
    echo -e "${YELLOW}   This will work but will be slower than NPU-accelerated systems${NC}"
    echo -e "${YELLOW}   Consider upgrading to Intel 12th gen+ with NPU for best performance${NC}"
    echo
    read -p "Continue with CPU-only installation? (y/N): " continue_cpu
    case $continue_cpu in
        [Yy]* )
            echo -e "${GREEN}Proceeding with CPU-only installation...${NC}"
            ;;
        * )
            echo -e "${RED}Installation cancelled. Consider upgrading hardware for optimal experience.${NC}"
            exit 1
            ;;
    esac
fi

if [ "$TOTAL_MEM_GB" -lt 8 ]; then
    echo -e "${RED}‚ùå Need at least 8GB RAM (found ${TOTAL_MEM_GB}GB)${NC}"
    echo -e "${YELLOW}üí° INT4-AWQ model is memory efficient but still needs 8GB+${NC}"
    exit 1
fi
if [ "$AVAILABLE_GB" -lt 15 ]; then
    echo -e "${RED}‚ùå Need at least 15GB disk space${NC}"
    exit 1
fi
echo -e "${GREEN}‚úÖ System OK: ${TOTAL_MEM_GB}GB RAM, ${AVAILABLE_GB}GB space${NC}"

# Install system dependencies
echo -e "\n${BLUE}üì¶ Installing system packages...${NC}"
sudo pacman -S --needed --noconfirm python python-pip base-devel cmake git intel-compute-runtime
echo -e "${GREEN}‚úÖ System packages installed${NC}"

# Create clean Python environment
echo -e "\n${BLUE}üêç Setting up Python environment...${NC}"
[ -d "npglue-env" ] && rm -rf npglue-env
python -m venv npglue-env
source npglue-env/bin/activate
pip install --upgrade pip wheel

# Install AI packages
echo -e "${BLUE}ü§ñ Installing AI packages (CPU-only, strict)...${NC}"
# 1. Install PyTorch CPU-only FIRST to establish preference
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
# 2. Install OpenVINO (Intel optimized, no CUDA)
pip install "openvino>=2024.0.0" "openvino-tokenizers>=2024.0.0"
# 3. Install base packages that should be safe
pip install fastapi uvicorn psutil huggingface-hub requests numpy packaging setuptools wheel
# 4. Install transformers carefully (check if it tries to pull CUDA)
echo -e "${CYAN}Installing transformers (watching for CUDA dependencies)...${NC}"
pip install transformers --no-deps
pip install tokenizers safetensors regex pyyaml tqdm
# 5. Install optimum packages
echo -e "${CYAN}Installing optimum-intel...${NC}"
pip install optimum --no-deps
pip install optimum-intel --no-deps
# 6. Install protobuf and tokenizer dependencies (required for many models)
echo -e "${CYAN}Installing protobuf and tokenizer dependencies...${NC}"
pip install protobuf
# Try to install sentencepiece, fall back to alternative if build fails
echo -e "${CYAN}Attempting to install sentencepiece (may require build tools)...${NC}"
pip install sentencepiece || {
    echo -e "${YELLOW}‚ö†Ô∏è  SentencePiece build failed, trying alternative tokenizer...${NC}"
    pip install transformers[sentencepiece] || {
        echo -e "${YELLOW}‚ö†Ô∏è  Installing basic tokenizers only (some models may need manual setup)${NC}"
        pip install tokenizers
    }
}
echo -e "${GREEN}‚úÖ AI packages installed (CPU-only)${NC}"

# Download model (safe, no loading)
echo -e "\n${BLUE}üì• Selecting AI model...${NC}"
echo -e "${CYAN}Choose your AI model:${NC}"
echo
echo -e "${GREEN}OpenVINO Pre-Optimized Models (fastest setup):${NC}"
echo -e "${YELLOW}1) Qwen3-8B-INT8${NC}         - Best quality, ~6-8GB, excellent for complex tasks"
echo -e "${YELLOW}2) Qwen3-0.6B-FP16${NC}       - Fast, ~1-2GB, good for simple tasks"
echo -e "${YELLOW}3) OpenLlama-7B-INT4${NC}     - Great balance, ~4-5GB, good for coding"
echo -e "${YELLOW}4) OpenLlama-3B-INT4${NC}     - Lightweight, ~2-3GB, fast responses"
echo
echo -e "${GREEN}Community Pre-Optimized Models:${NC}"
echo -e "${YELLOW}5) Llama-3.1-8B-INT4${NC}     - Latest Llama, ~5-6GB, excellent coding"
echo
echo -e "${GREEN}Convert-on-Install Models (requires conversion):${NC}"
echo -e "${YELLOW}6) Phi-3-Mini-4K${NC}         - Microsoft, ~4GB, excellent for NPU"
echo -e "${YELLOW}7) DeepSeek-Coder-6.7B${NC}   - Code specialist, ~6-7GB, best for coding"
echo -e "${YELLOW}8) DeepSeek-Coder-1.3B${NC}   - Light code specialist, ~2GB, fast coding"
echo -e "${RED}‚ö†Ô∏è  Note: OpenLlama models (3,4) require sentencepiece which may fail to build${NC}"
echo -e "${RED}   on some systems. Choose Qwen3, Phi-3, or DeepSeek for more reliability.${NC}"
echo
while true; do
    read -p "Enter your choice (1-8): " choice
    case $choice in
        1)
            MODEL_REPO="OpenVINO/Qwen3-8B-int8-ov"
            MODEL_DIR="models/qwen3-8b-int8"
            MODEL_FILE="$MODEL_DIR/openvino_model.bin"
            MODEL_NAME="Qwen3-8B-INT8"
            MODEL_SIZE="~6-8GB"
            MODEL_DESC="Best quality for complex tasks"
            MODEL_TYPE="preoptimized"
            MODEL_ID="qwen3-8b"
            break
            ;;
        2)
            MODEL_REPO="OpenVINO/Qwen3-0.6B-fp16-ov"
            MODEL_DIR="models/qwen3-0.6b-fp16"
            MODEL_FILE="$MODEL_DIR/openvino_model.bin"
            MODEL_NAME="Qwen3-0.6B-FP16"
            MODEL_SIZE="~1-2GB"
            MODEL_DESC="Fast and lightweight"
            MODEL_TYPE="preoptimized"
            MODEL_ID="qwen3-0.6b"
            break
            ;;
        3)
            MODEL_REPO="OpenVINO/open_llama_7b_v2-int4-ov"
            MODEL_DIR="models/open-llama-7b-int4"
            MODEL_FILE="$MODEL_DIR/openvino_model.bin"
            MODEL_NAME="OpenLlama-7B-INT4"
            MODEL_SIZE="~4-5GB"
            MODEL_DESC="Great balance of quality and speed"
            MODEL_TYPE="preoptimized"
            MODEL_ID="open-llama-7b"
            break
            ;;
        4)
            MODEL_REPO="OpenVINO/open_llama_3b_v2-int4-ov"
            MODEL_DIR="models/open-llama-3b-int4"
            MODEL_FILE="$MODEL_DIR/openvino_model.bin"
            MODEL_NAME="OpenLlama-3B-INT4"
            MODEL_SIZE="~2-3GB"
            MODEL_DESC="Lightweight with good performance"
            MODEL_TYPE="preoptimized"
            MODEL_ID="open-llama-3b"
            break
            ;;
        5)
            MODEL_REPO="Gunulhona/openvino-llama-3.1-8B_int4"
            MODEL_DIR="models/llama-3.1-8b-int4"
            MODEL_FILE="$MODEL_DIR/openvino_model.bin"
            MODEL_NAME="Llama-3.1-8B-INT4"
            MODEL_SIZE="~5-6GB"
            MODEL_DESC="Latest Llama with excellent coding abilities"
            MODEL_TYPE="preoptimized"
            MODEL_ID="llama-3.1-8b"
            break
            ;;
        6)
            MODEL_REPO="microsoft/Phi-3-mini-4k-instruct"
            MODEL_DIR="models/phi-3-mini-4k"
            MODEL_FILE="$MODEL_DIR/openvino_model.bin"
            MODEL_NAME="Phi-3-Mini-4K"
            MODEL_SIZE="~4GB"
            MODEL_DESC="Microsoft model optimized for NPU"
            MODEL_TYPE="convert"
            MODEL_ID="phi-3-mini"
            break
            ;;
        7)
            MODEL_REPO="deepseek-ai/deepseek-coder-6.7b-instruct"
            MODEL_DIR="models/deepseek-coder-6.7b"
            MODEL_FILE="$MODEL_DIR/openvino_model.bin"
            MODEL_NAME="DeepSeek-Coder-6.7B"
            MODEL_SIZE="~6-7GB"
            MODEL_DESC="Specialized coding model, excellent for development"
            MODEL_TYPE="convert"
            MODEL_ID="deepseek-coder-6.7b"
            break
            ;;
        8)
            MODEL_REPO="deepseek-ai/deepseek-coder-1.3b-instruct"
            MODEL_DIR="models/deepseek-coder-1.3b"
            MODEL_FILE="$MODEL_DIR/openvino_model.bin"
            MODEL_NAME="DeepSeek-Coder-1.3B"
            MODEL_SIZE="~2GB"
            MODEL_DESC="Lightweight coding specialist"
            MODEL_TYPE="convert"
            MODEL_ID="deepseek-coder-1.3b"
            break
            ;;
        *)
            echo -e "${RED}Please enter a number between 1 and 8${NC}"
            ;;
    esac
done

mkdir -p models
if [ -f "$MODEL_FILE" ]; then
    echo -e "${GREEN}‚úÖ $MODEL_NAME already downloaded${NC}"
else
    if [ "$MODEL_TYPE" = "preoptimized" ]; then
        echo -e "${CYAN}Downloading $MODEL_NAME OpenVINO model...${NC}"
        echo -e "${PURPLE}$MODEL_DESC - $MODEL_SIZE${NC}"
        python -c "
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id='$MODEL_REPO',
    local_dir='$MODEL_DIR',
    local_dir_use_symlinks=False
)
print('‚úÖ $MODEL_NAME downloaded successfully')
print('üöÄ Ready for NPU/GPU acceleration!')
print('üíæ Size: $MODEL_SIZE')
"
    else
        echo -e "${CYAN}Converting $MODEL_NAME to OpenVINO format...${NC}"
        echo -e "${PURPLE}$MODEL_DESC - $MODEL_SIZE${NC}"
        echo -e "${YELLOW}‚ö†Ô∏è  This will take 5-15 minutes depending on model size${NC}"
        python -c "
import os
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer

print('üì• Downloading original model from Hugging Face...')
tokenizer = AutoTokenizer.from_pretrained('$MODEL_REPO', trust_remote_code=True)
print('üîÑ Converting to OpenVINO format (this takes time)...')
model = OVModelForCausalLM.from_pretrained(
    '$MODEL_REPO', 
    export=True,
    device='CPU',
    trust_remote_code=True
)
print('üíæ Saving optimized model...')
os.makedirs('$MODEL_DIR', exist_ok=True)
model.save_pretrained('$MODEL_DIR')
tokenizer.save_pretrained('$MODEL_DIR')
print('‚úÖ $MODEL_NAME converted and optimized successfully!')
print('üöÄ Ready for NPU/GPU acceleration!')
print('üíæ Size: $MODEL_SIZE')
"
    fi
fi

# Save model choice for server (include model ID for proper API responses)
echo "MODEL_PATH=$MODEL_DIR" > .model_config
echo "MODEL_ID=$MODEL_ID" >> .model_config

# Memory-safe verification (no model loading)
echo -e "\n${BLUE}üß™ Verifying installation...${NC}"
python -c "
import openvino
import transformers
print('‚úÖ OpenVINO:', openvino.__version__)
print('‚úÖ Transformers:', transformers.__version__)
print('‚úÖ Available devices:', openvino.Core().available_devices)
print('‚úÖ All packages working correctly')
"

# Create startup script
cat > start_server.sh << 'EOF'
#!/bin/bash
cd "$(dirname "$0")"
source npglue-env/bin/activate
python server_production.py
EOF
chmod +x start_server.sh

# CPU optimization (ASK USER)
echo -e "\n${BLUE}‚ö° CPU Performance Optimization${NC}"
echo -e "${CYAN}Would you like to enable CPU performance mode for better AI inference?${NC}"
echo -e "${YELLOW}This temporarily sets CPU governor to 'performance' mode.${NC}"
echo -e "${YELLOW}Will be automatically restored when you exit the server.${NC}"
echo
read -p "Enable CPU performance mode? (y/N): " enable_cpu_boost
case $enable_cpu_boost in
    [Yy]* )
        echo -e "${GREEN}Enabling CPU performance mode...${NC}"
        sudo ./boost_cpu.sh
        ;;
    * )
        echo -e "${CYAN}Keeping current CPU settings${NC}"
        ;;
esac

# Beautiful completion message
clear
echo -e "${GREEN}‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ${NC}"
echo -e "${GREEN}‚îÇ           üéâ INSTALLATION COMPLETE!         ‚îÇ${NC}"
echo -e "${GREEN}‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ${NC}"
echo
echo -e "${BLUE}üìã NPGlue is ready! Here's what to do next:${NC}"
echo
echo -e "${PURPLE}‚úÖ Installed: $MODEL_NAME${NC}"
echo -e "${CYAN}   üìÅ Path: $MODEL_DIR${NC}"
echo -e "${CYAN}   üè∑Ô∏è  Model ID: $MODEL_ID${NC}"
echo -e "${CYAN}   üíæ Size: $MODEL_SIZE${NC}"
echo
echo -e "${YELLOW}1. Start the AI server:${NC}"
echo -e "   ${CYAN}./start_server.sh${NC}"
echo
echo -e "${YELLOW}2. Configure Goose (SAFE method):${NC}"
echo -e "   ${CYAN}# Check if you have existing Goose config:${NC}"
echo -e "   ${CYAN}ls ~/.config/goose/config.yaml${NC}"
echo
echo -e "   ${CYAN}# If NO existing config:${NC}"
echo -e "   ${CYAN}mkdir -p ~/.config/goose${NC}"
echo -e "   ${CYAN}cp goose_config_example.yaml ~/.config/goose/config.yaml${NC}"
echo
echo -e "   ${CYAN}# If you HAVE existing config, add these lines:${NC}"
echo -e "${PURPLE}   provider: ollama${NC}"
echo -e "${PURPLE}   model: $MODEL_ID${NC}"
echo -e "${PURPLE}   api_base: http://localhost:11434${NC}"
echo -e "${PURPLE}   # (don't overwrite your existing settings!)${NC}"
echo
echo -e "${YELLOW}3. Update Zed settings (WORKING SOLUTION):${NC}"
echo -e "   ${CYAN}# See zed_settings_example.json for complete config${NC}"
echo -e "   ${CYAN}# Key settings for your ~/.config/zed/settings.json:${NC}"
echo -e "${PURPLE}   \"language_models\": {${NC}"
echo -e "${PURPLE}     \"ollama\": {${NC}"
echo -e "${PURPLE}       \"api_url\": \"http://localhost:11434\",${NC}"
echo -e "${PURPLE}       \"available_models\": [${NC}"
echo -e "${PURPLE}         {${NC}"
echo -e "${PURPLE}           \"name\": \"$MODEL_ID\",${NC}"
echo -e "${PURPLE}           \"display_name\": \"$MODEL_NAME Local\",${NC}"
echo -e "${PURPLE}           \"max_tokens\": 4096,${NC}"
echo -e "${PURPLE}           \"supports_tools\": true${NC}"
echo -e "${PURPLE}         }${NC}"
echo -e "${PURPLE}       ]${NC}"
echo -e "${PURPLE}     }${NC}"
echo -e "${PURPLE}   },${NC}"
echo -e "${PURPLE}   \"agent\": {${NC}"
echo -e "${PURPLE}     \"default_model\": {${NC}"
echo -e "${PURPLE}       \"provider\": \"ollama\",${NC}"
echo -e "${PURPLE}       \"model\": \"$MODEL_ID\"${NC}"
echo -e "${PURPLE}     }${NC}"
echo -e "${PURPLE}   }${NC}"
echo
echo -e "${YELLOW}4. Test the installation:${NC}"
echo -e "   ${CYAN}curl http://localhost:11434/health${NC}"
echo
echo -e "${YELLOW}5. Switch models anytime:${NC}"
echo -e "   ${CYAN}./switch_model.sh${NC}"
echo
echo -e "${GREEN}üìä Performance expectations for $MODEL_NAME:${NC}"
if [ "$HAS_INTEL_NPU" = true ]; then
    echo -e "   ‚Ä¢ Speed: ${GREEN}20-30 tokens/sec${NC} (NPU accelerated)"
    echo -e "   ‚Ä¢ Hardware: ${GREEN}Intel NPU + OpenVINO optimized${NC}"
elif [ "$HAS_INTEL_GPU" = true ]; then
    echo -e "   ‚Ä¢ Speed: ${GREEN}5-10 tokens/sec${NC} (Intel GPU accelerated)"
    echo -e "   ‚Ä¢ Hardware: ${GREEN}Intel iGPU + OpenVINO optimized${NC}"
else
    echo -e "   ‚Ä¢ Speed: ${YELLOW}2-5 tokens/sec${NC} (CPU-only mode)"
    echo -e "   ‚Ä¢ Hardware: ${YELLOW}CPU-only (slower but functional)${NC}"
fi
echo -e "   ‚Ä¢ Memory: ${GREEN}$MODEL_SIZE${NC}"
echo -e "   ‚Ä¢ Model: ${GREEN}$MODEL_NAME - $MODEL_DESC${NC}"
echo -e "   ‚Ä¢ Quality: ${GREEN}95%+ of FP16 capability${NC}"
echo
echo -e "${BLUE}üí° Tips:${NC}"
echo -e "   ‚Ä¢ First generation is slower (warmup)"
echo -e "   ‚Ä¢ Use 'source npglue-env/bin/activate' to activate environment"
echo -e "   ‚Ä¢ Server runs on http://localhost:11434"
echo -e "   ‚Ä¢ Use ./switch_model.sh to change models without reinstalling"
echo
echo -e "${GREEN}‚ú® Happy coding with your local AI assistant! ‚ú®${NC}"
