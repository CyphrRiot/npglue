#!/bin/bash
# NPGlue - Qwen3 Installation Script
# Simple, beautiful, and memory-safe installation

set -e

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m'

clear
echo -e "${BLUE}â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®${NC}"
echo -e "${BLUE}â”‚             ğŸš€ NPGlue Installer             â”‚${NC}"
echo -e "${BLUE}â”‚        Qwen3 + OpenVINO + Goose          â”‚${NC}"
echo -e "${BLUE}â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯${NC}"
echo
echo -e "${CYAN}This will install everything you need for local AI coding assistance${NC}"
echo

# System checks
echo -e "${BLUE}ğŸ“‹ Checking system requirements...${NC}"
TOTAL_MEM_GB=$(free -g | awk '/^Mem:/{print $2}')
AVAILABLE_GB=$(df . | awk 'NR==2{printf "%.0f", $4/1024/1024}')

# Check for Intel NPU/iGPU
echo -e "${BLUE}ğŸ” Checking Intel hardware compatibility...${NC}"
HAS_INTEL_GPU=false
HAS_INTEL_NPU=false

# Check for Intel integrated graphics
if lspci | grep -i "intel.*graphics" >/dev/null 2>&1; then
    HAS_INTEL_GPU=true
    echo -e "${GREEN}âœ… Intel integrated graphics detected${NC}"
else
    echo -e "${YELLOW}âš ï¸  No Intel integrated graphics detected${NC}"
fi

# Check for Intel NPU (Neural Processing Unit)
if lspci | grep -i "intel.*npu\|intel.*neural" >/dev/null 2>&1; then
    HAS_INTEL_NPU=true
    echo -e "${GREEN}âœ… Intel NPU detected - hardware acceleration available${NC}"
elif [ -d "/sys/class/drm/renderD*" ] && ls /sys/class/drm/renderD* 2>/dev/null | grep -q .; then
    # Check for Intel compute units via render devices
    echo -e "${BLUE}ğŸ” Checking for Intel compute acceleration...${NC}"
    HAS_INTEL_GPU=true
    echo -e "${GREEN}âœ… Intel compute acceleration available${NC}"
else
    echo -e "${YELLOW}âš ï¸  No Intel NPU detected${NC}"
fi

# Provide guidance based on hardware
if [ "$HAS_INTEL_NPU" = true ]; then
    echo -e "${CYAN}ğŸ’¡ Your system has Intel NPU - excellent for AI acceleration!${NC}"
elif [ "$HAS_INTEL_GPU" = true ]; then
    echo -e "${CYAN}ğŸ’¡ Your system has Intel GPU - good for OpenVINO acceleration${NC}"
else
    echo -e "${YELLOW}ğŸ’¡ No Intel NPU/GPU detected - will use CPU-only mode${NC}"
    echo -e "${YELLOW}   This will work but will be slower than NPU-accelerated systems${NC}"
    echo -e "${YELLOW}   Consider upgrading to Intel 12th gen+ with NPU for best performance${NC}"
    echo
    read -p "Continue with CPU-only installation? (y/N): " continue_cpu
    case $continue_cpu in
        [Yy]* )
            echo -e "${GREEN}Proceeding with CPU-only installation...${NC}"
            ;;
        * )
            echo -e "${RED}Installation cancelled. Consider upgrading hardware for optimal experience.${NC}"
            exit 1
            ;;
    esac
fi

if [ "$TOTAL_MEM_GB" -lt 8 ]; then
    echo -e "${RED}âŒ Need at least 8GB RAM (found ${TOTAL_MEM_GB}GB)${NC}"
    echo -e "${YELLOW}ğŸ’¡ INT4-AWQ model is memory efficient but still needs 8GB+${NC}"
    exit 1
fi
if [ "$AVAILABLE_GB" -lt 15 ]; then
    echo -e "${RED}âŒ Need at least 15GB disk space${NC}"
    exit 1
fi
echo -e "${GREEN}âœ… System OK: ${TOTAL_MEM_GB}GB RAM, ${AVAILABLE_GB}GB space${NC}"

# Install system dependencies
echo -e "\n${BLUE}ğŸ“¦ Installing system packages...${NC}"
sudo pacman -S --needed --noconfirm python python-pip base-devel cmake git intel-compute-runtime
echo -e "${GREEN}âœ… System packages installed${NC}"

# Create clean Python environment
echo -e "\n${BLUE}ğŸ Setting up Python environment...${NC}"
[ -d "npglue-env" ] && rm -rf npglue-env
python -m venv npglue-env
source npglue-env/bin/activate
pip install --upgrade pip wheel

# Install AI packages
echo -e "${BLUE}ğŸ¤– Installing AI packages (CPU-only, strict)...${NC}"
# 1. Install PyTorch CPU-only FIRST to establish preference
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
# 2. Install OpenVINO (Intel optimized, no CUDA)
pip install "openvino>=2024.0.0" "openvino-tokenizers>=2024.0.0"
# 3. Install base packages that should be safe
pip install fastapi uvicorn psutil huggingface-hub requests numpy packaging setuptools wheel
# 4. Install transformers carefully (check if it tries to pull CUDA)
echo -e "${CYAN}Installing transformers (watching for CUDA dependencies)...${NC}"
pip install transformers --no-deps
pip install tokenizers safetensors regex pyyaml tqdm
# 5. Install optimum packages
echo -e "${CYAN}Installing optimum-intel...${NC}"
pip install optimum --no-deps  
pip install optimum-intel --no-deps
echo -e "${GREEN}âœ… AI packages installed (CPU-only)${NC}"

# Download model (safe, no loading)
echo -e "\n${BLUE}ğŸ“¥ Selecting Qwen3 model...${NC}"
echo -e "${CYAN}Choose your Qwen3 model variant:${NC}"
echo
echo -e "${YELLOW}1) Qwen3-8B-INT8${NC}   - Best quality, ~6-8GB, slower"
echo -e "${YELLOW}2) Qwen3-0.6B-FP16${NC} - Faster, ~1-2GB, good for simple tasks"
echo
while true; do
    read -p "Enter your choice (1 or 2): " choice
    case $choice in
        1)
            MODEL_REPO="OpenVINO/Qwen3-8B-int8-ov"
            MODEL_DIR="models/qwen3-8b-int8"
            MODEL_FILE="$MODEL_DIR/openvino_model.bin"
            MODEL_NAME="Qwen3-8B-INT8"
            MODEL_SIZE="~6-8GB"
            MODEL_DESC="Best quality for complex tasks"
            break
            ;;
        2)
            MODEL_REPO="OpenVINO/Qwen3-0.6B-fp16-ov"
            MODEL_DIR="models/qwen3-0.6b-fp16"
            MODEL_FILE="$MODEL_DIR/openvino_model.bin"
            MODEL_NAME="Qwen3-0.6B-FP16"
            MODEL_SIZE="~1-2GB"
            MODEL_DESC="Fast and lightweight"
            break
            ;;
        *)
            echo -e "${RED}Please enter 1 or 2${NC}"
            ;;
    esac
done

mkdir -p models
if [ -f "$MODEL_FILE" ]; then
    echo -e "${GREEN}âœ… $MODEL_NAME already downloaded${NC}"
else
    echo -e "${CYAN}Downloading $MODEL_NAME OpenVINO model...${NC}"
    echo -e "${PURPLE}$MODEL_DESC - $MODEL_SIZE${NC}"
    python -c "
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id='$MODEL_REPO',
    local_dir='$MODEL_DIR',
    local_dir_use_symlinks=False
)
print('âœ… $MODEL_NAME downloaded successfully')
print('ğŸš€ Much better for direct Q&A than DeepSeek-R1!')
print('ğŸ’¾ Size: $MODEL_SIZE')
"
fi

# Save model choice for server
echo "MODEL_PATH=$MODEL_DIR" > .model_config

# Memory-safe verification (no model loading)
echo -e "\n${BLUE}ğŸ§ª Verifying installation...${NC}"
python -c "
import openvino
import transformers
print('âœ… OpenVINO:', openvino.__version__)
print('âœ… Transformers:', transformers.__version__)
print('âœ… Available devices:', openvino.Core().available_devices)
print('âœ… All packages working correctly')
"

# Create startup script
cat > start_server.sh << 'EOF'
#!/bin/bash
cd "$(dirname "$0")"
source npglue-env/bin/activate
python server_production.py
EOF
chmod +x start_server.sh

# CPU optimization
echo -e "\n${BLUE}âš¡ Optimizing CPU performance...${NC}"
sudo ./boost_cpu.sh

# Beautiful completion message
clear
echo -e "${GREEN}â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®${NC}"
echo -e "${GREEN}â”‚           ğŸ‰ INSTALLATION COMPLETE!         â”‚${NC}"
echo -e "${GREEN}â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯${NC}"
echo
echo -e "${BLUE}ğŸ“‹ NPGlue is ready! Here's what to do next:${NC}"
echo
echo -e "${YELLOW}1. Start the AI server:${NC}"
echo -e "   ${CYAN}./start_server.sh${NC}"
echo
echo -e "${YELLOW}2. Configure Goose (SAFE method):${NC}"
echo -e "   ${CYAN}# Check if you have existing Goose config:${NC}"
echo -e "   ${CYAN}ls ~/.config/goose/config.yaml${NC}"
echo
echo -e "   ${CYAN}# If NO existing config:${NC}"
echo -e "   ${CYAN}mkdir -p ~/.config/goose${NC}"
echo -e "   ${CYAN}cp goose_config_example.yaml ~/.config/goose/config.yaml${NC}"
echo
echo -e "   ${CYAN}# If you HAVE existing config, add these lines:${NC}"
echo -e "${PURPLE}   provider: openai${NC}"
echo -e "${PURPLE}   model: qwen3${NC}"
echo -e "${PURPLE}   api_base: http://localhost:11434/v1${NC}"
echo -e "${PURPLE}   api_key: local-key${NC}"
echo -e "   ${CYAN}# (don't overwrite your existing settings!)${NC}"
echo
echo -e "${YELLOW}3. Update Zed settings (WORKING SOLUTION):${NC}"
echo -e "   ${CYAN}# Use Ollama provider (no API key hassles!):${NC}"
echo -e "${PURPLE}   \"language_models\": {${NC}"
echo -e "${PURPLE}     \"ollama\": {${NC}"
echo -e "${PURPLE}       \"api_url\": \"http://localhost:11434\",${NC}"
echo -e "${PURPLE}       \"available_models\": [${NC}"
echo -e "${PURPLE}         {${NC}"
echo -e "${PURPLE}           \"name\": \"qwen3\",${NC}"
echo -e "${PURPLE}           \"display_name\": \"Qwen3 Local\",${NC}"
echo -e "${PURPLE}           \"max_tokens\": 4096,${NC}"
echo -e "${PURPLE}           \"supports_tools\": true${NC}"
echo -e "${PURPLE}         }${NC}"
echo -e "${PURPLE}       ]${NC}"
echo -e "${PURPLE}     }${NC}"
echo -e "${PURPLE}   },${NC}"
echo -e "${PURPLE}   \"agent\": {${NC}"
echo -e "${PURPLE}     \"default_model\": {${NC}"
echo -e "${PURPLE}       \"provider\": \"ollama\",${NC}"
echo -e "${PURPLE}       \"model\": \"qwen3\"${NC}"
echo -e "${PURPLE}     }${NC}"
echo -e "${PURPLE}   }${NC}"
echo
echo -e "${YELLOW}4. Test the installation:${NC}"
echo -e "   ${CYAN}curl http://localhost:11434/health${NC}"
echo
echo -e "${GREEN}ğŸ“Š Performance expectations:${NC}"
if [ "$HAS_INTEL_NPU" = true ]; then
    echo -e "   â€¢ Speed: ${GREEN}30-70+ tokens/sec${NC} (NPU accelerated)"
    echo -e "   â€¢ Hardware: ${GREEN}Intel NPU + OpenVINO optimized${NC}"
elif [ "$HAS_INTEL_GPU" = true ]; then
    echo -e "   â€¢ Speed: ${GREEN}20-50+ tokens/sec${NC} (Intel GPU accelerated)"
    echo -e "   â€¢ Hardware: ${GREEN}Intel iGPU + OpenVINO optimized${NC}"
else
    echo -e "   â€¢ Speed: ${YELLOW}10-30 tokens/sec${NC} (CPU-only mode)"
    echo -e "   â€¢ Hardware: ${YELLOW}CPU-only (slower but functional)${NC}"
fi
echo -e "   â€¢ Memory: ${GREEN}~1-8GB${NC} (depends on model choice)"
echo -e "   â€¢ Model: ${GREEN}Qwen3 (your choice) - direct answers${NC}"
echo -e "   â€¢ Quality: ${GREEN}95%+ of FP16 capability${NC}"
echo
echo -e "${BLUE}ğŸ’¡ Tips:${NC}"
echo -e "   â€¢ First generation is slower (warmup)"
echo -e "   â€¢ Use 'source npglue-env/bin/activate' to activate environment"
echo -e "   â€¢ Server runs on http://localhost:11434"
echo
echo -e "${GREEN}âœ¨ Happy coding with your local AI assistant! âœ¨${NC}"
